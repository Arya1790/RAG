{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1idJRv4rfBg0_qd6LdlWioOP2xOxLdMsU",
      "authorship_tag": "ABX9TyMhwsj6bN3eVBmsb2OPL5Fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arya1790/RAG/blob/main/RAG_Application_Using_LangChain__Weviate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "libraries:-\n",
        "\n",
        "weaviate-client : vector database\n",
        "\n",
        "LangChain : framework to build with LLMs\n",
        "\n",
        "tiktoken : fast open-source tokenizer by OpenAI.\n",
        "It's reversible and lossless, so you can convert tokens back into the original text\n",
        "\n",
        "pypdf : free and open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files\n",
        "\n",
        "onnx : model 200% faster, which eliminates the need to use a GPU"
      ],
      "metadata": {
        "id": "bXsvCex3-YOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cVtCPu8t-fgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weaviate-client langchain tiktoken pypdf rapidocr-onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XJQupf-ZfH",
        "outputId": "a1947d99-1b99-4f13-d34c-794d218b7a6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: rapidocr-onnxruntime in /usr/local/lib/python3.10/dist-packages (1.3.24)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Requirement already satisfied: httpx<=0.27.0,>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.27.0)\n",
            "Requirement already satisfied: validators==0.33.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.33.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.8.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.65.1)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.65.1)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.65.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.23)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.93)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: pyclipper>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.3.0.post5)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (4.8.0.76)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.16.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (2.0.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (9.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from rapidocr-onnxruntime) (1.18.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.0)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client) (5.27.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (71.0.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<=0.27.0,>=0.25.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<=0.27.0,>=0.25.0->weaviate-client) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtvEqY5EwaBp",
        "outputId": "986adcc3-d07c-43c7-fbcc-67bf0eaba884"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.11)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.23 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.23)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.93)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WEAVIATE cluster name rag\n",
        "WEAVIATE_CLUSTER=\"https://ke7zibneqjq4wfbjhopcq.c0.asia-southeast1.gcp.weaviate.cloud\"\n",
        "WEAVIATE_API_KEY=\"NGDzCoSwS6mUtFPfqvemlyQ4zYIhNWvPNnLV\""
      ],
      "metadata": {
        "id": "aoBRqbwrk_le"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "\n",
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY\n",
        "\n",
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
        ")"
      ],
      "metadata": {
        "id": "6YiL9V02k_nw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing unicode error in google colab, the code snippet forces Python to use UTF-8 encoding as the preferred encoding, ensuring consistent text handling across different environments\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "FwayamlewgWD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify embedding model (using huggingface sentence transformer)\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=embedding_model_name\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3-V6xW9k_qK",
        "outputId": "8e33f149-ffc9-4681-ae76-cf7aef60fbcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJklVwxSk_sh",
        "outputId": "f66d40d9-4d72-467b-88f6-8b55563c84ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 31 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 89 0 (offset 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3IF4Lzwk_u_",
        "outputId": "a56b61e1-98c3-4604-d879-ae72d9d54ee5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez?,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;?New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their perfor-\\nmance lags behind task-speciﬁc architectures. Additionally, providing provenance\\nfor their decisions and updating their world knowledge remain open research prob-\\nlems. Pre-trained models with a differentiable access mechanism to explicit non-\\nparametric memory can overcome this issue, but have so far been only investigated\\nfor extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe\\nfor retrieval-augmented generation (RAG) — models which combine pre-trained\\nparametric and non-parametric memory for language generation. We introduce\\nRAG models where the parametric memory is a pre-trained seq2seq model and\\nthe non-parametric memory is a dense vector index of Wikipedia, accessed with\\na pre-trained neural retriever. We compare two RAG formulations, one which\\nconditions on the same retrieved passages across the whole generated sequence,\\nand another which can use different passages per token. We ﬁne-tune and evaluate\\nour models on a wide range of knowledge-intensive NLP tasks and set the state of\\nthe art on three open domain QA tasks, outperforming parametric seq2seq models\\nand task-speciﬁc retrieve-and-extract architectures. For language generation tasks,\\nwe ﬁnd that RAG models generate more speciﬁc, diverse and factual language than\\na state-of-the-art parametric-only seq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [ 8] with a differentiable retriever, have shown promising results,\\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text into chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Dbk2A_k_xV",
        "outputId": "ff9ac1d3-a569-4b01-ae9d-8ae20549c07b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez?,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;?New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their perfor-\\nmance lags behind task-speciﬁc architectures. Additionally, providing provenance\\nfor their decisions and updating their world knowledge remain open research prob-\\nlems. Pre-trained models with a differentiable access mechanism to explicit non-')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Weaviate.from_documents(\n",
        "    docs, embeddings, client=client, by_text=False\n",
        ")"
      ],
      "metadata": {
        "id": "5Ymqj267zSV8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "by_text=False: This parameter indicates whether the embeddings are to be indexed by text or not.\n",
        "When by_text=False, it means that the embeddings are directly provided and used as is, rather than being generated from the text within Weaviate."
      ],
      "metadata": {
        "id": "osbdPd1gbJ8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is rag?\", k=3)[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPQI8c8q0V0e",
        "outputId": "1d15bbe7-5627-4396-91ca-96af8e19b8f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cases for NQ, where an extractive model would score 0%.\n",
            "4.2 Abstractive Question Answering\n",
            "As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\n",
            "points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\n",
            "impressive given that (i) those models access gold passages with speciﬁc information required to\n",
            "generate the reference answer, (ii) many questions are unanswerable without the gold passages, and\n",
            "(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\n",
            "from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\n",
            "correct text more often than BART. Later, we also show that RAG generations are more diverse than\n",
            "BART generations (see §4.5).\n",
            "4.3 Jeopardy Question Generation\n",
            "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is rag?\", k=3)[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t13BUSd3zSY5",
        "outputId": "36a06b33-7fc1-4691-f592-084321fd6d65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimize the negative marginal log-likelihood of each target,P\n",
            "j\u0000logp(yj|xj)using stochastic\n",
            "gradient descent with Adam [ 28]. Updating the document encoder BERT dduring training is costly as\n",
            "it requires the document index to be periodically updated as REALM does during pre-training [ 20].\n",
            "We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and\n",
            "index) ﬁxed, only ﬁne-tuning the query encoder BERT qand the BART generator.\n",
            "2.5 Decoding\n",
            "At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x).\n",
            "RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\n",
            "tor with transition probability: p0\n",
            "✓(yi|x, y 1:i\u00001)=P\n",
            "z2top-k(p(·|x))p⌘(zi|x)p✓(yi|x, z i,y1:i\u00001)To\n",
            "decode, we can plug p0\n",
            "✓(yi|x, y 1:i\u00001)into a standard beam decoder.\n",
            "RAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_db.similarity_search(\"what is rag?\", k=3)[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZEJ85e2zSbq",
        "outputId": "e643f448-adad-41ba-f86e-c57d5bd8f2f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broader Impact\n",
            "This work offers several positive societal beneﬁts over previous work: the fact that it is more\n",
            "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\n",
            "with generations that are more factual, and offers more control and interpretability. RAG could be\n",
            "employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\n",
            "with a medical index and asking it open-domain questions on that topic, or by helping people be more\n",
            "effective at their jobs.\n",
            "With these advantages also come potential downsides: Wikipedia, or any potential external knowledge\n",
            "source, will probably never be entirely factual and completely devoid of bias. Since RAG can be\n",
            "employed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\n",
            "to a lesser extent, including that it might be used to generate abuse, faked or misleading content in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    vector_db.similarity_search(\n",
        "        \"what is attention?\", k=3)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JahHJhVVzSeH",
        "outputId": "aafd1739-3532-4536-88ce-790b60752835"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'page': 14, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv , abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366 .\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\\nAAAI Conference on Artiﬁcial Intelligence , 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329 .\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.'), Document(metadata={'page': 7, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8'), Document(metadata={'page': 7, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n1020304050KR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-TokRAG-Seq1020304050KR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-TokRAG-SeqFixed DPRBM251020304050KR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-LRAG-Tok B-1RAG-Seq R-LRAG-Seq B-1Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Hsmua710zSgO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnFZaff30gph",
        "outputId": "edfb50d2-9f19-46df-c4cf-d6014a9ea4d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from google.colab import userdata\n",
        "\n",
        "huggingfacehub_api_token=userdata.get('HF')"
      ],
      "metadata": {
        "id": "jXkJOUTF0nhW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=huggingfacehub_api_token,\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\":180}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y09JG81N0nj5",
        "outputId": "0e500755-fc88-46fe-80fa-51ba641aa9e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " temperature = 1 means that the model’s predictions are based on its learned probabilities without additional scaling"
      ],
      "metadata": {
        "id": "ecMQbOGVb-Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "PreBgGmP0nmj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnablePassthrough : In a chain or pipeline of operations sometimes you need a component that just passes the data along without altering it. This class/function can serve that purpose.\n",
        "\n",
        "StrOutputParser: output is consistently formatted as a string"
      ],
      "metadata": {
        "id": "7VLxC0iXcWrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "GgyHq-Ni0npW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_db.as_retriever()"
      ],
      "metadata": {
        "id": "F-rEd86J1r80"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "6voqHoHf1tUe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"what is rag system?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNZOJvU11uuW",
        "outputId": "a8a6a76b-8200-4787-d29c-bec0150b1b06"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: what is rag system?\n",
            "Context: [Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6'), Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer, (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,'), Document(metadata={'page': 9, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='Broader Impact\\nThis work offers several positive societal beneﬁts over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\neffective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in'), Document(metadata={'page': 7, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n1020304050KR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-TokRAG-Seq1020304050KR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-TokRAG-SeqFixed DPRBM251020304050KR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-LRAG-Tok B-1RAG-Seq R-LRAG-Seq B-1Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],\\nfact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our')]\n",
            "Answer:\n",
            "RAG (Retrieval-Augmented Generation) is a system that uses retrieval to improve the performance of various natural language processing tasks. It was developed by the authors of the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" and was tested on several benchmark datasets, including FEVER, Open MS-MARCO NLG, and Jeopardy question generation. RAG outperformed state-of-the-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_chain.invoke(\"How does the RAG model differ from traditional language generation models?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dtuviyC1w5O",
        "outputId": "cff70442-850a-49fe-9738-78f5e6acf900"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: You are an assistant for question-answering tasks.\n",
            "Use the following pieces of retrieved context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use ten sentences maximum and keep the answer concise.\n",
            "Question: How does the RAG model differ from traditional language generation models?\n",
            "Context: [Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='cases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer, (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,'), Document(metadata={'page': 3, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4'), Document(metadata={'page': 2, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='top K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence (y|x)⇡X\\nz2top-k(p(·|x))p⌘(z|x)p✓(y|x, z)=X\\nz2top-k(p(·|x))p⌘(z|x)NY\\nip✓(yi|x, z, y 1:i\\x001)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token (y|x)⇡NY\\niX\\nz2top-k(p(·|x))p⌘(z|x)p✓(yi|x, z i,y1:i\\x001)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class'), Document(metadata={'page': 5, 'source': '/content/drive/MyDrive/genAI/retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf'}, page_content='with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over\\n452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more\\nfactual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both\\nRAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of\\nRAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to\\nbe more speciﬁc by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows\\nan example. When generating “Sun”, the posterior is high for document 2 which mentions “The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is')]\n",
            "Answer:\n",
            "The RAG model differs from traditional language generation models in that it uses a retrieval-augmented approach to generate text. In this approach, the model retrieves relevant documents from a knowledge base and uses them to generate text. This allows the model to generate more factually correct text and to be more specific in its responses. The RAG model outperforms traditional language generation models on several benchmarks, including the MSMARCO NLG task and Jeopardy question generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = rag_chain.invoke(\"How does the RAG model differ from traditional language generation models?\")\n",
        "start  = output.find('Answer:')+len('Answer:')\n",
        "output[start:].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "h96rdIYw2Zva",
        "outputId": "ef2a8162-7bfb-421d-8d0f-88d7a68dadc6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The RAG model differs from traditional language generation models in that it uses a retrieval-augmented approach to generate text. In this approach, the model retrieves relevant documents from a knowledge base and uses them to generate text. This allows the model to generate more factually correct text and to be more specific in its responses. The RAG model outperforms traditional language generation models on several benchmarks, including the MSMARCO NLG task and Jeopardy question generation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMrnhK8q2ZyG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf, ppt, txt, xml, json, csv"
      ],
      "metadata": {
        "id": "qkbuMaTsk_0j"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}